{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense, Flatten\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KT Anna Skirt-FT.png\n",
      "KT Anna Skirt-PKT.png\n",
      "KT Anna Skirt-WB-X.png\n",
      "KT Anna Skirt-WB.png\n",
      "KT Jojo Sweatshirt-BK.png\n",
      "KT Jojo Sweatshirt-FT.png\n",
      "KT Jojo Sweatshirt-NKBND.png\n",
      "KT Jojo Sweatshirt-PKT.png\n",
      "KT Jojo Sweatshirt-SLV.png\n",
      "KT Olivia Skirt-BK.png\n",
      "KT Olivia Skirt-FT.png\n",
      "KT Rumi Sweatshirt-Cuff.png\n",
      "KT Rumi Sweatshirt-FT.png\n",
      "KT Rumi Sweatshirt-NKBDG.png\n",
      "KT Rumi Sweatshirt-NKBND.png\n",
      "KT Rumi Sweatshirt-SLV.png\n",
      "KT Rumi Sweatshirt-WB.png\n",
      "KT longsleeve crew neck-FT.png\n",
      "KT longsleeve crew neck-SLV.png\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "all_filenames = listdir('../Spline_Out/Pics/')\n",
    "all_filenames.sort()\n",
    "all_filenames.remove('.DS_Store')\n",
    "all_filenames.remove('.ipynb_checkpoints')\n",
    "for filename in all_filenames:\n",
    "    print(filename)\n",
    "    images.append(img_to_array(load_img('../Spline_Out/Pics/'+filename, target_size=(299, 299))))\n",
    "images = np.array(images, dtype=float)\n",
    "images = preprocess_input(images)\n",
    "\n",
    "IR2 = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "#IR2.load_weights('/data/models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "features = IR2.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 19\n"
     ]
    }
   ],
   "source": [
    "# We will cap each input sequence to 100 tokens\n",
    "max_caption_len = 100\n",
    "# Initialize the function that will create our vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "\n",
    "# Read a document and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Load all the HTML files\n",
    "X = []\n",
    "all_filenames = listdir('../Spline_Out/XML/')\n",
    "all_filenames.sort()\n",
    "all_filenames.remove('.DS_Store')\n",
    "all_filenames.remove('.ipynb_checkpoints')\n",
    "for filename in all_filenames:\n",
    "    X.append(load_doc('../Spline_Out/XML/'+filename))\n",
    "\n",
    "# Create the vocabulary from the html files\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Add +1 to leave space for empty words\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Translate each word in text file to the matching vocabulary index\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "# The longest HTML file\n",
    "max_length = max(len(s) for s in sequences)\n",
    "\n",
    "\n",
    "print(len(sequences), len(features))\n",
    "\n",
    "\n",
    "\n",
    "# Intialize our final input to the model\n",
    "X, y, image_data = list(), list(), list()\n",
    "for img_no, seq in enumerate(sequences):\n",
    "    for i in range(1, len(seq)):\n",
    "        # Add the entire sequence to the input and only keep the next word for the output\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # If the sentence is shorter than max_length, fill it up with empty words\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # Map the output to one-hot encoding\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # Add and image corresponding to the HTML file\n",
    "        image_data.append(features[img_no])\n",
    "        # Cut the input sentence to 100 tokens, and add it to the input data\n",
    "        X.append(in_seq[-100:])\n",
    "        y.append(out_seq)\n",
    "\n",
    "X, y, image_data = np.array(X), np.array(y), np.array(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6654, 2048)\n",
      "(6654, 100)\n",
      "(6654, 703)\n",
      "(19, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(image_data.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     140600      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 256)     467968      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          262272      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100, 256)     525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 100, 128)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 128)     32896       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 256)     0           repeat_vector_1[0][0]            \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 100, 512)     1574912     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 512)          2099200     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 703)          360639      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,463,799\n",
      "Trainable params: 5,463,799\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_features = Input(shape=(2048,))\n",
    "image_flat = Dense(128, activation='relu')(image_features)\n",
    "ir2_out = RepeatVector(max_caption_len)(image_flat)\n",
    "\n",
    "language_input = Input(shape=(max_caption_len,))\n",
    "language_model = Embedding(vocab_size, 200, input_length=max_caption_len)(language_input)\n",
    "language_model = LSTM(256, return_sequences=True)(language_model)\n",
    "language_model = LSTM(256, return_sequences=True)(language_model)\n",
    "language_model = TimeDistributed(Dense(128, activation='relu'))(language_model)\n",
    "\n",
    "decoder = concatenate([ir2_out, language_model])\n",
    "decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "decoder_output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "model = Model(inputs=[image_features, language_input], outputs=decoder_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train model\n",
    "filepath=\"org-weights-epoch-{epoch:04d}---loss-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=15)\n",
    "#stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     140600      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 256)     467968      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          262272      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100, 256)     525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 100, 128)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 128)     32896       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 256)     0           repeat_vector_1[0][0]            \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 100, 512)     1574912     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 512)          2099200     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 703)          360639      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,463,799\n",
      "Trainable params: 5,463,799\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model(RNN).json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"org-weights-epoch-0075---loss-0.0039.hdf5\")\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5988 samples, validate on 666 samples\n",
      "Epoch 1/120\n",
      "5988/5988 [==============================] - 321s 54ms/step - loss: 0.2518 - val_loss: 2.4831\n",
      "Epoch 2/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.2067 - val_loss: 2.4665\n",
      "Epoch 3/120\n",
      "5988/5988 [==============================] - 315s 53ms/step - loss: 0.1974 - val_loss: 2.5137\n",
      "Epoch 4/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.1599 - val_loss: 2.5116\n",
      "Epoch 5/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.1545 - val_loss: 2.5171\n",
      "Epoch 6/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.1430 - val_loss: 2.6844\n",
      "Epoch 7/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.1389 - val_loss: 2.5131\n",
      "Epoch 8/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.1904 - val_loss: 2.4522\n",
      "Epoch 9/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.1236 - val_loss: 2.6757\n",
      "Epoch 10/120\n",
      "5988/5988 [==============================] - 318s 53ms/step - loss: 0.1108 - val_loss: 2.5682\n",
      "Epoch 11/120\n",
      "5988/5988 [==============================] - 319s 53ms/step - loss: 0.0990 - val_loss: 2.6596\n",
      "Epoch 12/120\n",
      "5988/5988 [==============================] - 315s 53ms/step - loss: 0.1014 - val_loss: 2.6727\n",
      "Epoch 13/120\n",
      "5988/5988 [==============================] - 317s 53ms/step - loss: 0.0940 - val_loss: 2.6755\n",
      "Epoch 14/120\n",
      "5988/5988 [==============================] - 322s 54ms/step - loss: 0.0745 - val_loss: 2.6437\n",
      "Epoch 15/120\n",
      "5988/5988 [==============================] - 317s 53ms/step - loss: 0.0697 - val_loss: 2.7139\n",
      "\n",
      "Epoch 00015: saving model to org-weights-epoch-0015---loss-0.0697.hdf5\n",
      "Epoch 16/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0596 - val_loss: 2.7325\n",
      "Epoch 17/120\n",
      "5988/5988 [==============================] - 308s 51ms/step - loss: 0.0683 - val_loss: 2.8205\n",
      "Epoch 18/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0624 - val_loss: 2.8018\n",
      "Epoch 19/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0469 - val_loss: 2.8090\n",
      "Epoch 20/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0407 - val_loss: 2.7885\n",
      "Epoch 21/120\n",
      "5988/5988 [==============================] - 308s 51ms/step - loss: 0.0417 - val_loss: 2.9485\n",
      "Epoch 22/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0388 - val_loss: 2.8449\n",
      "Epoch 23/120\n",
      "5988/5988 [==============================] - 315s 53ms/step - loss: 0.0304 - val_loss: 2.7820\n",
      "Epoch 24/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0338 - val_loss: 2.8563\n",
      "Epoch 25/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0274 - val_loss: 2.8625\n",
      "Epoch 26/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0323 - val_loss: 2.8771\n",
      "Epoch 27/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0255 - val_loss: 2.9324\n",
      "Epoch 28/120\n",
      "5988/5988 [==============================] - 315s 53ms/step - loss: 0.0213 - val_loss: 2.8910\n",
      "Epoch 29/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0239 - val_loss: 2.9414\n",
      "Epoch 30/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0198 - val_loss: 3.0037\n",
      "\n",
      "Epoch 00030: saving model to org-weights-epoch-0030---loss-0.0198.hdf5\n",
      "Epoch 31/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.0228 - val_loss: 2.8527\n",
      "Epoch 32/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0226 - val_loss: 2.9849\n",
      "Epoch 33/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0144 - val_loss: 2.9370\n",
      "Epoch 34/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0197 - val_loss: 2.9358\n",
      "Epoch 35/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0176 - val_loss: 2.9931\n",
      "Epoch 36/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.0195 - val_loss: 2.9352\n",
      "Epoch 37/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0148 - val_loss: 3.0716\n",
      "Epoch 38/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0198 - val_loss: 2.9986\n",
      "Epoch 39/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0145 - val_loss: 3.0125\n",
      "Epoch 40/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0099 - val_loss: 3.0098\n",
      "Epoch 41/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0143 - val_loss: 3.0759\n",
      "Epoch 42/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0159 - val_loss: 2.9975\n",
      "Epoch 43/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0093 - val_loss: 3.0995\n",
      "Epoch 44/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.0137 - val_loss: 2.9630\n",
      "Epoch 45/120\n",
      "5988/5988 [==============================] - 316s 53ms/step - loss: 0.0103 - val_loss: 2.9973\n",
      "\n",
      "Epoch 00045: saving model to org-weights-epoch-0045---loss-0.0103.hdf5\n",
      "Epoch 46/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0111 - val_loss: 2.8923\n",
      "Epoch 47/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0084 - val_loss: 3.0224\n",
      "Epoch 48/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0070 - val_loss: 3.0438\n",
      "Epoch 49/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0062 - val_loss: 3.1007\n",
      "Epoch 50/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0128 - val_loss: 3.1546\n",
      "Epoch 51/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0039 - val_loss: 3.1556\n",
      "Epoch 52/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0046 - val_loss: 3.1637\n",
      "Epoch 53/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0029 - val_loss: 3.1043\n",
      "Epoch 54/120\n",
      "5988/5988 [==============================] - 309s 52ms/step - loss: 0.0028 - val_loss: 3.1448\n",
      "Epoch 55/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0090 - val_loss: 3.1919\n",
      "Epoch 56/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.0065 - val_loss: 3.2201\n",
      "Epoch 57/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0041 - val_loss: 3.2042\n",
      "Epoch 58/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0055 - val_loss: 3.2649\n",
      "Epoch 59/120\n",
      "5988/5988 [==============================] - 317s 53ms/step - loss: 0.0138 - val_loss: 3.2140\n",
      "Epoch 60/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0061 - val_loss: 3.1873\n",
      "\n",
      "Epoch 00060: saving model to org-weights-epoch-0060---loss-0.0061.hdf5\n",
      "Epoch 61/120\n",
      "5988/5988 [==============================] - 308s 52ms/step - loss: 0.0036 - val_loss: 3.1578\n",
      "Epoch 62/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0106 - val_loss: 3.1225\n",
      "Epoch 63/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0043 - val_loss: 3.1040\n",
      "Epoch 64/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0139 - val_loss: 3.0950\n",
      "Epoch 65/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0057 - val_loss: 3.1065\n",
      "Epoch 66/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0026 - val_loss: 3.1215\n",
      "Epoch 67/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0056 - val_loss: 3.1516\n",
      "Epoch 68/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0023 - val_loss: 3.1282\n",
      "Epoch 69/120\n",
      "5988/5988 [==============================] - 310s 52ms/step - loss: 0.0053 - val_loss: 3.1808\n",
      "Epoch 70/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0015 - val_loss: 3.2159\n",
      "Epoch 71/120\n",
      "5988/5988 [==============================] - 314s 52ms/step - loss: 0.0040 - val_loss: 3.2466\n",
      "Epoch 72/120\n",
      "5988/5988 [==============================] - 312s 52ms/step - loss: 0.0036 - val_loss: 3.1762\n",
      "Epoch 73/120\n",
      "5988/5988 [==============================] - 308s 51ms/step - loss: 0.0031 - val_loss: 3.2291\n",
      "Epoch 74/120\n",
      "5988/5988 [==============================] - 311s 52ms/step - loss: 0.0042 - val_loss: 3.2319\n",
      "Epoch 75/120\n",
      "5988/5988 [==============================] - 313s 52ms/step - loss: 0.0039 - val_loss: 3.2507\n",
      "\n",
      "Epoch 00075: saving model to org-weights-epoch-0075---loss-0.0039.hdf5\n",
      "Epoch 76/120\n",
      " 704/5988 [==>...........................] - ETA: 4:25 - loss: 5.4992e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7c8319909cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "loaded_model.fit([image_data, X], y, batch_size=64, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to Notebook\n"
     ]
    }
   ],
   "source": [
    "#model_json = model.to_json()\n",
    "#with open(\"model(RNN).json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "#print(\"Saved Model to Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'START'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(900):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0][-100:]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # Print the prediction\n",
    "        print(' ' + word, end='')\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'END':\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot identify image file '../Spline_Out/TestPics/KT-3012-V2-NKBND.val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d9905f0f1cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and image, preprocess it for IR2, extract features and generate the HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Spline_Out/TestPics/KT-3012-V2-NKBND.val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIR2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[1;32m    347\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    348\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2570\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m     raise IOError(\"cannot identify image file %r\"\n\u001b[0;32m-> 2572\u001b[0;31m                   % (filename if filename else fp))\n\u001b[0m\u001b[1;32m   2573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot identify image file '../Spline_Out/TestPics/KT-3012-V2-NKBND.val'"
     ]
    }
   ],
   "source": [
    "# Load and image, preprocess it for IR2, extract features and generate the HTML\n",
    "test_image = img_to_array(load_img('../Spline_Out/TestPics/KT-3012-V2-NKBND.val', target_size=(299, 299)))\n",
    "test_image = np.array(test_image, dtype=float)\n",
    "test_image = preprocess_input(test_image)\n",
    "test_features = IR2.predict(np.array([test_image]))\n",
    "generate_desc(loaded_model, tokenizer, np.array(test_features), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
